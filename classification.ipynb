{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "\n",
    "import seaborn as sns\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file (replace this path with your actual file path)\n",
    "file_path = '/Users/gowthamkishorevijay/Desktop/Playground/projects/my-venv/CEAS_08.csv'  # Adjust based on your local environment\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    return urls\n",
    "\n",
    "# Function to clean email body\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "       text = ''\n",
    "    \n",
    "\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'>+=+=+=+=+', '', text)  # Removing separators like '+=+=+=+=+=+'\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and digits\n",
    "    \n",
    "    # 3. Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 4. Remove non-alphabetic characters and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 5. Remove stopwords\n",
    "    text_tokens = text.split()\n",
    "    filtered_words = [word for word in text_tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply URL extraction and data cleaning\n",
    "df['extracted_urls'] = df['body'].apply(extract_urls)\n",
    "df['body'] = df['body'].apply(clean_text)\n",
    "\n",
    "\n",
    "\n",
    "print(df[['sender', 'subject', 'body', 'extracted_urls', 'label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['body', 'extracted_urls']]\n",
    "y = df['label']  # Ensure y is numeric for regression tasks\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessor: handling 'body' and 'extracted_urls' separately\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('body', Pipeline([\n",
    "            ('convert_to_str', FunctionTransformer(lambda x: x.astype(str), validate=False)),\n",
    "            ('tfidf', TfidfVectorizer(min_df=1, max_df=0.9))\n",
    "        ]), 'body'),\n",
    "        ('extracted_urls', Pipeline([\n",
    "            ('convert_to_str', FunctionTransformer(lambda x: x.astype(str), validate=False)),\n",
    "            ('tfidf', TfidfVectorizer())\n",
    "        ]), 'extracted_urls'),\n",
    "        # For 'message_length' column: calculate the length of the message body\n",
    "        ('message_length', Pipeline([\n",
    "            ('length', FunctionTransformer(lambda X: np.array(X.apply(len)).reshape(-1, 1))),  # Reshape to 2D\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), 'body')  \n",
    "    ])\n",
    "\n",
    "# Create the full pipeline with Linear Regression\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lin_reg', LinearRegression())\n",
    "])\n",
    "\n",
    "# Define hyperparameter distributions without 'normalize'\n",
    "param_dist = {\n",
    "    'lin_reg__fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# Use random search to find the best hyperparameters\n",
    "rand_search = RandomizedSearchCV(model, \n",
    "                                 param_distributions=param_dist, \n",
    "                                 n_iter=2,  # Adjusted to match available combinations\n",
    "                                 cv=5, \n",
    "                                 random_state=42)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = rand_search.predict(X_train)\n",
    "y_pred_test = rand_search.predict(X_test)\n",
    "\n",
    "# Evaluate your model\n",
    "print(\"Train Set Performance:\")\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_train, y_pred_train)}\")\n",
    "print(f\"R^2 Score: {r2_score(y_train, y_pred_train)}\")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred_test)}\")\n",
    "print(f\"R^2 Score: {r2_score(y_test, y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df[['body', 'extracted_urls']]\n",
    "y = df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessor: handling 'body' and 'extracted_urls' separately\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('body', Pipeline([\n",
    "            ('convert_to_str', FunctionTransformer(lambda x: x.astype(str), validate=False)),\n",
    "            ('tfidf', TfidfVectorizer(min_df=1, max_df=0.9))\n",
    "        ]), 'body'),\n",
    "        ('extracted_urls', Pipeline([\n",
    "            ('convert_to_str', FunctionTransformer(lambda x: x.astype(str), validate=False)),\n",
    "            ('tfidf', TfidfVectorizer())\n",
    "        ]), 'extracted_urls'),\n",
    "         # For 'message_length' column: calculate the length of the message body\n",
    "        ('message_length', Pipeline([\n",
    "            ('length', FunctionTransformer(lambda X: np.array(X.apply(len)).reshape(-1, 1)))  # Reshape to 2D\n",
    "        ]), 'body')  \n",
    "    ])\n",
    "\n",
    "# Create the full pipeline with MultinomialNB\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Define hyperparameter distributions (alpha parameter for smoothing in MultinomialNB)\n",
    "param_dist = {\n",
    "    'nb__alpha': [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "}\n",
    "\n",
    "# Use random search to find the best hyperparameters\n",
    "rand_search = RandomizedSearchCV(model, \n",
    "                                 param_distributions=param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5, \n",
    "                                 random_state=42)  # Added random_state for reproducibility\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = rand_search.predict(X_train)\n",
    "y_pred_test = rand_search.predict(X_test)\n",
    "\n",
    "# Evaluate your model\n",
    "print(\"Train Set Performance:\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and labels\n",
    "\n",
    "\n",
    "## For Random forest classifier\n",
    "X = df[['body', 'extracted_urls']]\n",
    "y = df['label']\n",
    "\n",
    "def message_length(X):\n",
    "    return X.apply(len)\n",
    "\n",
    "\n",
    "def column_as_string(X):\n",
    "    return X.astype(str)\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessor: handling 'body' and 'extracted_urls' separately\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('body', Pipeline([\n",
    "            ('convert_to_str', FunctionTransformer(lambda x: x.astype(str), validate=False)),\n",
    "            ('tfidf', TfidfVectorizer(min_df=1, max_df=0.9))\n",
    "        ]), 'body'),\n",
    "        ('extracted_urls', Pipeline([\n",
    "            ('convert_to_str', FunctionTransformer(lambda x: x.astype(str), validate=False)),\n",
    "            ('tfidf', TfidfVectorizer())\n",
    "        ]), 'extracted_urls'),\n",
    "         # For 'message_length' column: calculate the length of the message body\n",
    "       ('message_length', Pipeline([\n",
    "            ('length', FunctionTransformer(lambda X: np.array(X.apply(len)).reshape(-1, 1)))  # Reshape to 2D\n",
    "        ]), 'body')  \n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Create the full pipeline with RandomForestClassifier\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Define hyperparameter distributions\n",
    "param_dist = {\n",
    "    'rf__n_estimators': randint(50, 500),  # Use randint for a distribution of values\n",
    "    'rf__max_depth': randint(1, 20)        # Use randint for a distribution of values\n",
    "}\n",
    "\n",
    "# Use random search to find the best hyperparameters\n",
    "rand_search = RandomizedSearchCV(model, \n",
    "                                 param_distributions=param_dist, \n",
    "                                 n_iter=6, \n",
    "                                 cv=5, \n",
    "                                 random_state=42)  # Added random_state for reproducibility\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = rand_search.predict(X_train)\n",
    "y_pred_test = rand_search.predict(X_test)\n",
    "\n",
    "# Evaluate your model\n",
    "print(\"Train Set Performance:\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_train = rand_search.predict_proba(X_train)[:,1]\n",
    "pred_prob_test = rand_search.predict_proba(X_test)[:,1]\n",
    "\n",
    "# calculate ROC AUC score\n",
    "roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "print(\"\\nTrain ROC AUC:\", roc_auc_train)\n",
    "print(\"Test ROC AUC:\", roc_auc_test)\n",
    "\n",
    "# plot the ROC curve\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, pred_prob_train)\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, pred_prob_test)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.plot(fpr_train, tpr_train, label=\"Train ROC AUC: {:.2f}\".format(roc_auc_train))\n",
    "plt.plot(fpr_test, tpr_test, label=\"Test ROC AUC: {:.2f}\".format(roc_auc_test))\n",
    "plt.legend()\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()\n",
    "\n",
    "# calculate confusion matrix\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(11,4))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "sns.heatmap(cm_train, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"Oranges\", fmt='.4g', ax=ax[0])\n",
    "ax[0].set_xlabel(\"Predicted Label\")\n",
    "ax[0].set_ylabel(\"True Label\")\n",
    "ax[0].set_title(\"Train Confusion Matrix\")\n",
    "\n",
    "sns.heatmap(cm_test, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"Oranges\", fmt='.4g', ax=ax[1])\n",
    "ax[1].set_xlabel(\"Predicted Label\")\n",
    "ax[1].set_ylabel(\"True Label\")\n",
    "ax[1].set_title(\"Test Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_email_body = \"\"\"hey mate....\"\"\"\n",
    "\n",
    "# Apply the same preprocessing steps (extract URLs and clean text)\n",
    "new_email_body = clean_text(new_email_body)  # Clean the email body\n",
    "new_email_urls = extract_urls(new_email_body)  # Extract URLs\n",
    "new_email_length = len(new_email_body)  # Calculate the length of the email body\n",
    "\n",
    "# Create a DataFrame similar to the one used for training\n",
    "new_email_df = pd.DataFrame({\n",
    "    'body': [new_email_body],  # Cleaned email body\n",
    "    'extracted_urls': [' '.join(new_email_urls)],  # Join extracted URLs into a single string\n",
    "    'message_length': [new_email_length]  # Include the message length\n",
    "})\n",
    "\n",
    "# Make a prediction using the model pipeline\n",
    "prediction = rand_search.predict(new_email_df)\n",
    "\n",
    "# Print the prediction result\n",
    "if prediction == 0:\n",
    "    print(\"This is not a spam Email!\")\n",
    "else:\n",
    "    print(\"This is a Spam Email!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
